{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机数种子已设置为: 42\n",
      "当前设备: cuda\n",
      "--- 步骤1: 正在加载和预处理数据 ---\n",
      "正在将时间序列转换为监督学习格式...\n",
      "数据准备完成。\n",
      "\n",
      "--- 步骤2: 配置PSO优化器 ---\n",
      "\n",
      "--- 步骤3: 开始运行PSO ---\n",
      "--- 开始PSO优化 ---\n",
      "\n",
      "==================== PSO迭代: 1/50 ====================\n",
      "\n",
      "--- [迭代 1/50 | 粒子 1/100] 正在评估参数: {'CNN_filters': 177, 'cnn_kernel_size': 1, 'LSTM_layers': 2, 'units': 81, 'BiGRU_layers': 3} ---\n",
      "随机数种子已设置为: 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torchinfo import summary\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 0. 设置随机数种子\n",
    "# =============================================================================\n",
    "def set_seed(seed):\n",
    "    \"\"\"设置随机种子以保证结果可复现\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"随机数种子已设置为: {seed}\")\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "set_seed(GLOBAL_SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. PSO 算法实现\n",
    "# =============================================================================\n",
    "class Particle:\n",
    "    \"\"\"\n",
    "    代表粒子群中的单个粒子。\n",
    "    \"\"\"\n",
    "    def __init__(self, param_space):\n",
    "        self.param_space = param_space\n",
    "        self.dims = len(param_space)\n",
    "        \n",
    "        # 初始化位置和速度\n",
    "        self.position = np.array([random.uniform(low, high) for low, high, _ in param_space.values()])\n",
    "        self.velocity = np.array([random.uniform(-(high-low)/2, (high-low)/2) for low, high, _ in param_space.values()])\n",
    "        \n",
    "        # 初始化个体最优\n",
    "        self.pbest_position = self.position.copy()\n",
    "        self.pbest_score = float('inf')\n",
    "\n",
    "    def update_velocity(self, gbest_position, w=0.5, c1=1.5, c2=1.5):\n",
    "        \"\"\"更新粒子速度\"\"\"\n",
    "        r1 = np.random.rand(self.dims)\n",
    "        r2 = np.random.rand(self.dims)\n",
    "        \n",
    "        cognitive_velocity = c1 * r1 * (self.pbest_position - self.position)\n",
    "        social_velocity = c2 * r2 * (gbest_position - self.position)\n",
    "        \n",
    "        self.velocity = w * self.velocity + cognitive_velocity + social_velocity\n",
    "\n",
    "    def update_position(self):\n",
    "        \"\"\"更新粒子位置并确保其在边界内\"\"\"\n",
    "        self.position = self.position + self.velocity\n",
    "        \n",
    "        # 边界检查\n",
    "        for i, key in enumerate(self.param_space):\n",
    "            low, high, _ = self.param_space[key]\n",
    "            self.position[i] = np.clip(self.position[i], low, high)\n",
    "\n",
    "    def get_params_dict(self):\n",
    "        \"\"\"将粒子的位置向量转换为适用于模型的超参数字典\"\"\"\n",
    "        params = {}\n",
    "        for i, key in enumerate(self.param_space):\n",
    "            _, _, param_type = self.param_space[key]\n",
    "            # 根据参数类型（如整数）进行转换\n",
    "            if param_type == 'int':\n",
    "                value = int(round(self.position[i]))\n",
    "                # 确保kernel_size至少为1\n",
    "                if key == 'cnn_kernel_size' and value < 1:\n",
    "                    value = 1\n",
    "                params[key] = value\n",
    "            else: # float\n",
    "                params[key] = self.position[i]\n",
    "        return params\n",
    "\n",
    "class PSOOptimizer:\n",
    "    \"\"\"\n",
    "    粒子群优化器主类。\n",
    "    \"\"\"\n",
    "    def __init__(self, objective_function, param_space, n_particles, max_iter, **kwargs):\n",
    "        self.objective_function = objective_function\n",
    "        self.param_space = param_space\n",
    "        self.n_particles = n_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.kwargs = kwargs # 用于传递额外参数给目标函数，如data_loaders\n",
    "        \n",
    "        self.gbest_position = None\n",
    "        self.gbest_score = float('inf')\n",
    "        \n",
    "        # 初始化粒子群\n",
    "        self.swarm = [Particle(param_space) for _ in range(n_particles)]\n",
    "\n",
    "    def optimize(self):\n",
    "        print(\"--- 开始PSO优化 ---\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            print(f\"\\n{'='*20} PSO迭代: {i+1}/{self.max_iter} {'='*20}\")\n",
    "            \n",
    "            for j, particle in enumerate(self.swarm):\n",
    "                # 1. 获取当前粒子的超参数\n",
    "                current_params = particle.get_params_dict()\n",
    "                print(f\"\\n--- [迭代 {i+1}/{self.max_iter} | 粒子 {j+1}/{self.n_particles}] 正在评估参数: {current_params} ---\")\n",
    "                \n",
    "                # 2. 运行目标函数（模型训练与评估），获取适应度分数\n",
    "                score = self.objective_function(current_params, **self.kwargs)\n",
    "                print(f\"--- [迭代 {i+1}/{self.max_iter} | 粒子 {j+1}/{self.n_particles}] 评估得分 (验证损失): {score:.6f} ---\")\n",
    "                \n",
    "                # 3. 更新个体最优\n",
    "                if score < particle.pbest_score:\n",
    "                    particle.pbest_score = score\n",
    "                    particle.pbest_position = particle.position.copy()\n",
    "                    print(f\"新个体最优! 粒子 {j+1} 更新得分: {score:.6f}\")\n",
    "                \n",
    "                # 4. 更新全局最优\n",
    "                if score < self.gbest_score:\n",
    "                    self.gbest_score = score\n",
    "                    self.gbest_position = particle.position.copy()\n",
    "                    print(f\"\\n{'*'*20} 新全局最优! {'*'*20}\")\n",
    "                    print(f\"全局最优得分: {self.gbest_score:.6f}\")\n",
    "                    print(f\"全局最优参数: {self.get_best_params_dict()}\")\n",
    "                    print(f\"{'*'*50}\\n\")\n",
    "\n",
    "            # 5. 更新所有粒子的速度和位置\n",
    "            if self.gbest_position is not None:\n",
    "                for particle in self.swarm:\n",
    "                    particle.update_velocity(self.gbest_position)\n",
    "                    particle.update_position()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(\"\\n--- PSO优化完成 ---\")\n",
    "        print(f\"总耗时: {(end_time - start_time)/3600:.2f} 小时\")\n",
    "        print(f\"最佳验证损失: {self.gbest_score:.6f}\")\n",
    "        print(f\"最佳超参数: {self.get_best_params_dict()}\")\n",
    "        \n",
    "        return self.get_best_params_dict(), self.gbest_score\n",
    "\n",
    "    def get_best_params_dict(self):\n",
    "        \"\"\"将最佳位置向量转换为超参数字典\"\"\"\n",
    "        params = {}\n",
    "        for i, key in enumerate(self.param_space):\n",
    "            _, _, param_type = self.param_space[key]\n",
    "            if param_type == 'int':\n",
    "                value = int(round(self.gbest_position[i]))\n",
    "                if key == 'cnn_kernel_size' and value < 1:\n",
    "                    value = 1\n",
    "                params[key] = value\n",
    "            else:\n",
    "                params[key] = self.gbest_position[i]\n",
    "        return params\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 文件路径\n",
    "# =============================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前设备: {device}\")\n",
    "\n",
    "\n",
    "HIGH_FREQ_DATA_PATH = r''\n",
    "LOW_FREQ_DATA_PATH = r''\n",
    "ORIGINAL_DATA_PATH = r''\n",
    "MODEL_FILE_PATH = 'pso_best_model.pth' # PSO找到的最佳模型保存路径\n",
    "\n",
    "# --- 固定超参数 ---\n",
    "N_STEPS_IN, N_STEPS_OUT = 96, 24\n",
    "TARGET_STEPS = [3, 6, 12, 24]\n",
    "HIGH_FREQ_FEATURES = 1\n",
    "WEATHER_FEATURES = 5\n",
    "LOW_FREQ_FEATURES = 1\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.2\n",
    "TRANSFORMER_LAYERS = 2\n",
    "RELATIVE_POSITION_BUCKETS = 32\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 数据预处理函数\n",
    "# =============================================================================\n",
    "def time_series_to_supervised_mimo(data, n_in=96, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if isinstance(data, list) else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    orig_names = df.columns\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s(t-%d)' % (orig_names[j], i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        names += [('%s(t%s%d)' % (orig_names[j], '' if i==0 else '+', i)) for j in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 模型架构 \n",
    "# =============================================================================\n",
    "# --- 模块 1: 相对位置偏置生成器---\n",
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, num_buckets, max_distance, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.num_heads = num_heads\n",
    "        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.num_heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            ret += (n < 0).to(torch.long) * num_buckets\n",
    "            n = torch.abs(n)\n",
    "        else:\n",
    "            n = torch.max(n, torch.zeros_like(n))\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "        val_if_large = max_exact + (torch.log(n.float() / max_exact) / np.log(max_distance / max_exact) * (num_buckets - max_exact)).to(torch.long)\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, seq_len, device):\n",
    "        q_pos = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "        k_pos = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "        rel_pos = k_pos[None, :] - q_pos[:, None]\n",
    "        rp_bucket = self._relative_position_bucket(rel_pos, bidirectional=True, num_buckets=self.num_buckets, max_distance=self.max_distance)\n",
    "        bias = self.relative_attention_bias(rp_bucket)\n",
    "        return bias.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# --- 模块 2: UV RPE Transformer块---\n",
    "class UV_ScaledRPE_Block(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert self.head_dim * self.num_heads == self.d_model, \"embed_dim must be divisible by num_heads\"\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model))\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.rpe_scale_u = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
    "        self.rpe_scale_v = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
    "    def forward(self, x, bias):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        B, L, D = x.shape\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        final_bias = (bias * self.rpe_scale_u) + (bias * self.rpe_scale_v)\n",
    "        attn_scores = attn_scores + final_bias\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        attn_output = torch.matmul(attn_probs, v).transpose(1, 2).contiguous().view(B, L, D)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        x = residual + self.dropout1(attn_output)\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + self.dropout2(x)\n",
    "        return x\n",
    "\n",
    "# --- 模块 3: 高频分支---\n",
    "class EnhancedHighFrequencyModel(nn.Module):\n",
    "    def __init__(self, power_dim, weather_dim, embed_dim, cnn_filters, cnn_kernel_size, n_gru_layers, num_heads, dropout, n_transformer_blocks, num_pos_buckets):\n",
    "        super().__init__()\n",
    "        # 确保kernel_size为奇数，以方便对称填充\n",
    "        if cnn_kernel_size % 2 == 0:\n",
    "            cnn_kernel_size +=1\n",
    "        padding = (cnn_kernel_size - 1) // 2\n",
    "\n",
    "        self.power_conv = nn.Conv1d(in_channels=power_dim, out_channels=cnn_filters, kernel_size=cnn_kernel_size, padding=padding)\n",
    "        self.power_bigru = nn.GRU(cnn_filters, embed_dim, n_gru_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.power_fc = nn.Linear(embed_dim * 2, embed_dim)\n",
    "\n",
    "        self.weather_bigru = nn.GRU(weather_dim, embed_dim, 1, batch_first=True, bidirectional=True)\n",
    "        self.weather_fc = nn.Linear(embed_dim * 2, embed_dim)\n",
    "\n",
    "        self.relative_pos_bias_generator = RelativePositionBias(num_buckets=num_pos_buckets, max_distance=N_STEPS_IN, num_heads=num_heads)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(n_transformer_blocks):\n",
    "            block = nn.ModuleDict({\n",
    "                'uv_scaled_rpe_self_attn': UV_ScaledRPE_Block(d_model=embed_dim, num_heads=num_heads, d_ff=embed_dim*4, dropout=dropout),\n",
    "                'norm_cross_attn': nn.LayerNorm(embed_dim),\n",
    "                'cross_attn': nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True),\n",
    "                'dropout_cross_attn': nn.Dropout(dropout),\n",
    "                'norm_ffn_cross': nn.LayerNorm(embed_dim),\n",
    "                'ffn_cross': nn.Sequential(nn.Linear(embed_dim, embed_dim * 4), nn.GELU(), nn.Dropout(dropout), nn.Linear(embed_dim * 4, embed_dim)),\n",
    "                'dropout_ffn_cross': nn.Dropout(dropout)\n",
    "            })\n",
    "            self.transformer_blocks.append(block)\n",
    "\n",
    "    def forward(self, x_high_freq, x_weather):\n",
    "        h_power_conv = self.power_conv(x_high_freq.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        h_power_gru = self.power_bigru(h_power_conv)[0]\n",
    "        h_power = self.power_fc(h_power_gru)\n",
    "        h_weather = self.weather_fc(self.weather_bigru(x_weather)[0])\n",
    "        \n",
    "        relative_bias = self.relative_pos_bias_generator(h_power.size(1), device=h_power.device)\n",
    "\n",
    "        processed_power = h_power\n",
    "        for block in self.transformer_blocks:\n",
    "            processed_power = block['uv_scaled_rpe_self_attn'](processed_power, relative_bias)\n",
    "            residual = processed_power\n",
    "            norm_power_for_cross = block['norm_cross_attn'](processed_power)\n",
    "            cross_attn_output, _ = block['cross_attn'](query=norm_power_for_cross, key=h_weather, value=h_weather)\n",
    "            processed_power = residual + block['dropout_cross_attn'](cross_attn_output)\n",
    "            residual = processed_power\n",
    "            norm_power_for_ffn = block['norm_ffn_cross'](processed_power)\n",
    "            ffn_output = block['ffn_cross'](norm_power_for_ffn)\n",
    "            processed_power = residual + block['dropout_ffn_cross'](ffn_output)\n",
    "            \n",
    "        return processed_power[:, -1, :]\n",
    "\n",
    "# --- 低频分支 ---\n",
    "class LowFrequencyLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_lstm_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, embed_dim, n_lstm_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, x_low_freq):\n",
    "        _, (h_n, _) = self.lstm(x_low_freq)\n",
    "        return h_n[-1]\n",
    "\n",
    "# --- 顶层模型---\n",
    "class ICIAL_UV_RPE_Model(nn.Module):\n",
    "    def __init__(self, target_steps_names, power_dim, weather_dim, low_freq_dim, \n",
    "                 embed_dim, n_gru_layers, n_lstm_layers, cnn_filters, cnn_kernel_size, # 可调参数\n",
    "                 num_heads, dropout, transformer_layers, num_pos_buckets): # 固定参数\n",
    "        super().__init__()\n",
    "        # 验证 embed_dim 是否能被 num_heads 整除\n",
    "        if embed_dim % num_heads != 0:\n",
    "            # 调整 embed_dim 为最接近的、能被 num_heads 整除的数\n",
    "            embed_dim = round(embed_dim / num_heads) * num_heads\n",
    "            print(f\"警告: embed_dim ({params['units']}) 不能被 num_heads ({num_heads}) 整除。已自动调整为 {embed_dim}。\")\n",
    "        \n",
    "        self.target_steps_names = target_steps_names\n",
    "        self.quantiles_to_predict = sorted(list(set([0.5, 0.025, 0.975, 0.05, 0.95, 0.1, 0.9, 0.15, 0.85, 0.2, 0.8, 0.075, 0.925])))\n",
    "        \n",
    "        self.high_freq_branch = EnhancedHighFrequencyModel(power_dim, weather_dim, embed_dim, cnn_filters, cnn_kernel_size, n_gru_layers, num_heads, dropout, transformer_layers, num_pos_buckets)\n",
    "        self.low_freq_branch = LowFrequencyLSTM(low_freq_dim, embed_dim, n_lstm_layers, dropout)\n",
    "        \n",
    "        self.fusion_mlp = nn.Sequential(nn.Linear(embed_dim * 2, embed_dim * 2), nn.GELU(), nn.Dropout(dropout))\n",
    "        \n",
    "        self.quantile_output_heads = nn.ModuleDict()\n",
    "        for name in self.target_steps_names:\n",
    "            self.quantile_output_heads[name] = nn.Linear(embed_dim * 2, len(self.quantiles_to_predict))\n",
    "\n",
    "    def forward(self, x_high, x_weather, x_low):\n",
    "        shared_high_freq_features = self.high_freq_branch(x_high, x_weather)\n",
    "        shared_low_freq_features = self.low_freq_branch(x_low)\n",
    "        combined_features = torch.cat([shared_high_freq_features, shared_low_freq_features], dim=1)\n",
    "        final_features = self.fusion_mlp(combined_features)\n",
    "        return_dict = {name: self.quantile_output_heads[name](final_features) for name in self.target_steps_names}\n",
    "        return return_dict\n",
    "\n",
    "# --- 分位数损失函数 ---\n",
    "def quantile_loss(y_true, y_pred_quantiles, quantiles_to_predict):\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles_to_predict):\n",
    "        y_pred_q = y_pred_quantiles[:, i]\n",
    "        errors = y_true - y_pred_q\n",
    "        loss = torch.max(q * errors, (q - 1) * errors)\n",
    "        losses.append(loss.mean())\n",
    "    return torch.stack(losses).sum()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. PSO 目标函数\n",
    "# =============================================================================\n",
    "def objective_function(params, train_loader, val_loader, device):\n",
    "    \"\"\"\n",
    "    PSO的目标函数。接收一组超参数，返回模型在该超参数下的最小验证损失。\n",
    "    \"\"\"\n",
    "    set_seed(GLOBAL_SEED) # 保证每次评估的独立性和可复现性\n",
    "    \n",
    "    try:\n",
    "        # 动态调整 embed_dim (即 units) 以确保可以被 num_heads 整除\n",
    "        embed_dim = params['units']\n",
    "        if embed_dim % NUM_HEADS != 0:\n",
    "            embed_dim = int(round(embed_dim / NUM_HEADS) * NUM_HEADS)\n",
    "            if embed_dim == 0: embed_dim = NUM_HEADS\n",
    "        \n",
    "        model = ICIAL_UV_RPE_Model(\n",
    "            target_steps_names=[f't_plus_{s}' for s in TARGET_STEPS],\n",
    "            power_dim=HIGH_FREQ_FEATURES, weather_dim=WEATHER_FEATURES, low_freq_dim=LOW_FREQ_FEATURES,\n",
    "            # --- 使用PSO传递的可调参数 ---\n",
    "            embed_dim=embed_dim, \n",
    "            n_gru_layers=params['BiGRU_layers'],\n",
    "            n_lstm_layers=params['LSTM_layers'],\n",
    "            cnn_filters=params['CNN_filters'],\n",
    "            cnn_kernel_size=params['cnn_kernel_size'],\n",
    "            # --- 使用固定的超参数 ---\n",
    "            num_heads=NUM_HEADS, \n",
    "            dropout=DROPOUT, \n",
    "            transformer_layers=TRANSFORMER_LAYERS,\n",
    "            num_pos_buckets=RELATIVE_POSITION_BUCKETS\n",
    "        ).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"模型实例化失败! 参数: {params}, 错误: {e}\")\n",
    "        return float('inf') # 返回一个很大的值表示此参数组合无效\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # --- 缩短版的训练/验证循环 ---\n",
    "    EVAL_EPOCHS = 10   # 每个粒子评估时只跑10轮\n",
    "    EVAL_PATIENCE = 3  # 早停也更敏感\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    model_quantiles = model.quantiles_to_predict\n",
    "\n",
    "    for epoch in range(EVAL_EPOCHS):\n",
    "        model.train()\n",
    "        for x_high, x_weather, x_low, *y_targets in train_loader:\n",
    "            x_high, x_weather, x_low = x_high.to(device), x_weather.to(device), x_low.to(device)\n",
    "            y_targets_on_device = [y.to(device) for y in y_targets]\n",
    "            optimizer.zero_grad()\n",
    "            predictions_dict = model(x_high, x_weather, x_low)\n",
    "            batch_loss = 0.0\n",
    "            for i, name in enumerate(model.target_steps_names):\n",
    "                batch_loss += quantile_loss(y_targets_on_device[i], predictions_dict[name], model_quantiles)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_high, x_weather, x_low, *y_targets in val_loader:\n",
    "                x_high, x_weather, x_low = x_high.to(device), x_weather.to(device), x_low.to(device)\n",
    "                y_targets_on_device = [y.to(device) for y in y_targets]\n",
    "                predictions_dict = model(x_high, x_weather, x_low)\n",
    "                batch_val_loss = 0.0\n",
    "                for i, name in enumerate(model.target_steps_names):\n",
    "                    batch_val_loss += quantile_loss(y_targets_on_device[i], predictions_dict[name], model_quantiles)\n",
    "                val_loss += batch_val_loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= EVAL_PATIENCE:\n",
    "                # print(f\"提前停止于 epoch {epoch+1}，最佳验证损失: {best_val_loss:.4f}\")\n",
    "                break\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# =============================================================================\n",
    "# 6. 主执行脚本\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- 步骤 1: 数据加载与准备 (只执行一次) ---\n",
    "    print(\"--- 步骤1: 正在加载和预处理数据 ---\")\n",
    "    try:\n",
    "        high_freq_df = pd.read_csv(HIGH_FREQ_DATA_PATH).interpolate()\n",
    "        low_freq_df = pd.read_csv(LOW_FREQ_DATA_PATH).interpolate()\n",
    "        original_df = pd.read_csv(ORIGINAL_DATA_PATH).interpolate()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"错误：找不到数据文件！请在脚本中更新文件路径。{e}\")\n",
    "        exit()\n",
    "\n",
    "    weather_df = original_df[['Temp','Humidity','GHI','DHI','Rainfall']]\n",
    "    power_df = original_df[['Power']]\n",
    "\n",
    "    print(\"正在将时间序列转换为监督学习格式...\")\n",
    "    processed_power = time_series_to_supervised_mimo(power_df, N_STEPS_IN, N_STEPS_OUT)\n",
    "    processed_high = time_series_to_supervised_mimo(high_freq_df, N_STEPS_IN, N_STEPS_OUT)\n",
    "    processed_low = time_series_to_supervised_mimo(low_freq_df, N_STEPS_IN, N_STEPS_OUT)\n",
    "    processed_weather = time_series_to_supervised_mimo(weather_df, N_STEPS_IN, N_STEPS_OUT)\n",
    "\n",
    "    common_index = processed_power.index.intersection(processed_high.index).intersection(processed_low.index).intersection(processed_weather.index)\n",
    "    processed_power, processed_high, processed_low, processed_weather = [df.loc[common_index] for df in [processed_power, processed_high, processed_low, processed_weather]]\n",
    "    \n",
    "    y_cols = [f'Power(t{\"\" if s-1==0 else f\"+{s-1}\"})' for s in TARGET_STEPS] \n",
    "    y = processed_power[y_cols].values\n",
    "\n",
    "    X_high = processed_high[[c for c in processed_high.columns if '(t-' in c]].values.reshape(-1, N_STEPS_IN, HIGH_FREQ_FEATURES)\n",
    "    X_weather = processed_weather[[c for c in processed_weather.columns if '(t-' in c]].values.reshape(-1, N_STEPS_IN, WEATHER_FEATURES)\n",
    "    X_low = processed_low[[c for c in processed_low.columns if '(t-' in c]].values.reshape(-1, N_STEPS_IN, LOW_FREQ_FEATURES)\n",
    "\n",
    "    train_size = int(len(y) * 0.8); val_size = int(len(y) * 0.15)\n",
    "    def split_data(data): return data[:train_size], data[train_size:train_size+val_size], data[train_size+val_size:]\n",
    "    train_X_high, val_X_high, test_X_high = split_data(X_high)\n",
    "    train_X_weather, val_X_weather, test_X_weather = split_data(X_weather)\n",
    "    train_X_low, val_X_low, test_X_low = split_data(X_low)\n",
    "    train_y, val_y, test_y = split_data(y)\n",
    "\n",
    "    scaler_high = MinMaxScaler(); scaler_weather = MinMaxScaler(); scaler_low = MinMaxScaler()\n",
    "    def scale_3d_data(train, val, test, scaler):\n",
    "        train_s = scaler.fit_transform(train.reshape(-1, train.shape[-1])).reshape(train.shape)\n",
    "        val_s = scaler.transform(val.reshape(-1, val.shape[-1])).reshape(val.shape)\n",
    "        test_s = scaler.transform(test.reshape(-1, test.shape[-1])).reshape(test.shape)\n",
    "        return train_s, val_s, test_s\n",
    "    train_X_high_s, val_X_high_s, test_X_high_s = scale_3d_data(train_X_high, val_X_high, test_X_high, scaler_high)\n",
    "    train_X_weather_s, val_X_weather_s, test_X_weather_s = scale_3d_data(train_X_weather, val_X_weather, test_X_weather, scaler_weather)\n",
    "    train_X_low_s, val_X_low_s, test_X_low_s = scale_3d_data(train_X_low, val_X_low, test_X_low, scaler_low)\n",
    "\n",
    "    train_y_s_list, val_y_s_list = [], []\n",
    "    for i, s in enumerate(TARGET_STEPS):\n",
    "        scaler = MinMaxScaler()\n",
    "        train_y_s_list.append(scaler.fit_transform(train_y[:, i:i+1]).flatten())\n",
    "        val_y_s_list.append(scaler.transform(val_y[:, i:i+1]).flatten())\n",
    "\n",
    "    train_y_tensors = [torch.from_numpy(v).float() for v in train_y_s_list]\n",
    "    val_y_tensors = [torch.from_numpy(v).float() for v in val_y_s_list]\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(train_X_high_s).float(), torch.from_numpy(train_X_weather_s).float(), torch.from_numpy(train_X_low_s).float(), *train_y_tensors)\n",
    "    val_data = TensorDataset(torch.from_numpy(val_X_high_s).float(), torch.from_numpy(val_X_weather_s).float(), torch.from_numpy(val_X_low_s).float(), *val_y_tensors)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "    print(\"数据准备完成。\")\n",
    "\n",
    "    # --- 步骤 2: 定义PSO搜索空间和参数 ---\n",
    "    print(\"\\n--- 步骤2: 配置PSO优化器 ---\")\n",
    "    # 格式: 'param_name': (min_value, max_value, type)\n",
    "    param_space = {\n",
    "        'CNN_filters': (30, 260, 'int'),\n",
    "        'cnn_kernel_size': (1, 4, 'int'), # CNN_kernel_size in paper, use cnn_ for var name\n",
    "        'LSTM_layers': (1, 4, 'int'),\n",
    "        'units': (30, 260, 'int'), # Unified units for LSTM and BiGRU\n",
    "        'BiGRU_layers': (1, 4, 'int'),\n",
    "    }\n",
    "\n",
    "    # PSO超参数\n",
    "    N_PARTICLES = 100  # 粒子数量\n",
    "    MAX_ITER = 100     # 迭代次数\n",
    "\n",
    "    # --- 步骤 3: 实例化并运行PSO优化器 ---\n",
    "    print(\"\\n--- 步骤3: 开始运行PSO ---\")\n",
    "    pso = PSOOptimizer(\n",
    "        objective_function=objective_function,\n",
    "        param_space=param_space,\n",
    "        n_particles=N_PARTICLES,\n",
    "        max_iter=MAX_ITER,\n",
    "        # 传递额外固定参数给目标函数\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    best_params, best_score = pso.optimize()\n",
    "    \n",
    "    # --- 步骤 4: 显示最终结果 ---\n",
    "    print(\"\\n\\n=============================================\")\n",
    "    print(\"           PSO 优化最终结果\")\n",
    "    print(\"=============================================\")\n",
    "    print(f\"找到的最佳验证损失: {best_score}\")\n",
    "    print(\"找到的最佳超参数组合 (对应论文表格中的 Optimal Value):\")\n",
    "    print(f\"  - CNN filters: {best_params.get('CNN_filters')}\")\n",
    "    print(f\"  - CNN kernel_size: {best_params.get('cnn_kernel_size')}\")\n",
    "    print(f\"  - LSTM layers: {best_params.get('LSTM_layers')}\")\n",
    "    print(f\"  - LSTM units: {best_params.get('units')}\")\n",
    "    print(f\"  - BiGRU layers: {best_params.get('BiGRU_layers')}\")\n",
    "    print(f\"  - BiGRU units: {best_params.get('units')}\") # BiGRU和LSTM共享units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PVTimeSeries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
